{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting crunch-cli\n",
      "  Downloading crunch_cli-1.5.2-py3-none-any.whl (19 kB)\n",
      "Collecting click (from crunch-cli)\n",
      "  Using cached click-8.1.3-py3-none-any.whl (96 kB)\n",
      "Requirement already satisfied: requests in /home/ptl/internship/all_machine_learning/.venv/lib/python3.10/site-packages (from crunch-cli) (2.31.0)\n",
      "Collecting gitignorefile (from crunch-cli)\n",
      "  Downloading gitignorefile-1.1.2.tar.gz (12 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting coloredlogs (from crunch-cli)\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m739.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /home/ptl/internship/all_machine_learning/.venv/lib/python3.10/site-packages (from crunch-cli) (4.65.0)\n",
      "Collecting astor (from crunch-cli)\n",
      "  Downloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
      "Collecting joblib (from crunch-cli)\n",
      "  Downloading joblib-1.3.1-py3-none-any.whl (301 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting pyarrow (from crunch-cli)\n",
      "  Downloading pyarrow-12.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.9/38.9 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting pandas (from crunch-cli)\n",
      "  Downloading pandas-2.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: psutil in /home/ptl/internship/all_machine_learning/.venv/lib/python3.10/site-packages (from crunch-cli) (5.9.5)\n",
      "Collecting requirements-parser (from crunch-cli)\n",
      "  Downloading requirements_parser-0.5.0-py3-none-any.whl (18 kB)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->crunch-cli)\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /home/ptl/internship/all_machine_learning/.venv/lib/python3.10/site-packages (from pandas->crunch-cli) (2.8.2)\n",
      "Collecting pytz>=2020.1 (from pandas->crunch-cli)\n",
      "  Using cached pytz-2023.3-py2.py3-none-any.whl (502 kB)\n",
      "Collecting tzdata>=2022.1 (from pandas->crunch-cli)\n",
      "  Using cached tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /home/ptl/internship/all_machine_learning/.venv/lib/python3.10/site-packages (from pandas->crunch-cli) (1.23.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ptl/internship/all_machine_learning/.venv/lib/python3.10/site-packages (from requests->crunch-cli) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ptl/internship/all_machine_learning/.venv/lib/python3.10/site-packages (from requests->crunch-cli) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ptl/internship/all_machine_learning/.venv/lib/python3.10/site-packages (from requests->crunch-cli) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ptl/internship/all_machine_learning/.venv/lib/python3.10/site-packages (from requests->crunch-cli) (2023.5.7)\n",
      "Collecting types-setuptools>=57.0.0 (from requirements-parser->crunch-cli)\n",
      "  Downloading types_setuptools-68.0.0.0-py3-none-any.whl (44 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /home/ptl/internship/all_machine_learning/.venv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->crunch-cli) (1.16.0)\n",
      "Building wheels for collected packages: gitignorefile\n",
      "  Building wheel for gitignorefile (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gitignorefile: filename=gitignorefile-1.1.2-py3-none-any.whl size=6672 sha256=eb14ab7ed1982a3bae89badf76337b3aee9b9d914e7f1a9a8d31c3288bccc139\n",
      "  Stored in directory: /home/ptl/.cache/pip/wheels/6d/dc/c7/5875272151d55a37a44c163d1bddb1cdfff35278b82dcf4ba7\n",
      "Successfully built gitignorefile\n",
      "Installing collected packages: types-setuptools, pytz, gitignorefile, tzdata, requirements-parser, pyarrow, joblib, humanfriendly, click, astor, pandas, coloredlogs, crunch-cli\n",
      "Successfully installed astor-0.8.1 click-8.1.3 coloredlogs-15.0.1 crunch-cli-1.5.2 gitignorefile-1.1.2 humanfriendly-10.0 joblib-1.3.1 pandas-2.0.3 pyarrow-12.0.1 pytz-2023.3 requirements-parser-0.5.0 types-setuptools-68.0.0.0 tzdata-2023.3\n"
     ]
    }
   ],
   "source": [
    "!pip3 install crunch-cli --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded inline runner with module: <module '__main__'>\n",
      "extract shrill-mohamed/main.py\n",
      "extract shrill-mohamed/notebook.ipynb\n",
      "extract shrill-mohamed/requirements.txt\n",
      "download data/X_train.parquet from https://datacrunch-com.s3.eu-west-1.amazonaws.com/production/adialab/data-releases/1/X_train.parquet\n",
      "download data/y_train.parquet from https://datacrunch-com.s3.eu-west-1.amazonaws.com/production/adialab/data-releases/1/y_train.parquet\n",
      "download data/X_test.parquet from https://datacrunch-com.s3.eu-west-1.amazonaws.com/production/adialab/data-releases/1/X_test_reduced.parquet\n",
      "                                                                                \n",
      "---\n",
      "Success! Your environment has been correctly setup.\n",
      "Next recommended actions:\n",
      " - To get inside your workspace directory, run: cd shrill-mohamed\n",
      " - To see all of the available commands of the CrunchDAO CLI, run: crunch --help\n",
      "/home/ptl/internship/all_machine_learning/ranking/LambdaMart/shrill-mohamed\n"
     ]
    }
   ],
   "source": [
    "!crunch --notebook setup shrill-mohamed --token RKWcWWZlh0F23CITn46MN7sZUyzNQa5m4mBwP6cFcR1wc5HJSYW7wBS5AnvTI23e\n",
    "%cd shrill-mohamed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded inline runner with module: <module '__main__'>\n"
     ]
    }
   ],
   "source": [
    "import crunch\n",
    "crunch = crunch.load_notebook(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ptl/internship/all_machine_learning/ranking/LambdaMart/shrill-mohamed\n",
      "download data/X_train.parquet from https://datacrunch-com.s3.eu-west-1.amazonaws.com/production/adialab/data-releases/1/X_train.parquet\n",
      "already exists: file length match\n",
      "download data/y_train.parquet from https://datacrunch-com.s3.eu-west-1.amazonaws.com/production/adialab/data-releases/1/y_train.parquet\n",
      "already exists: file length match\n",
      "download data/X_test.parquet from https://datacrunch-com.s3.eu-west-1.amazonaws.com/production/adialab/data-releases/1/X_test_reduced.parquet\n",
      "already exists: file length match\n"
     ]
    }
   ],
   "source": [
    "%cd shrill-mohamed\n",
    "X_train, y_train, X_test = crunch.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_orig = X_train.copy()\n",
    "y_train_orig = y_train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "slicing = int(X_train.date.max()*0.8)\n",
    "slicing_index = X_train[X_train[\"date\"]==slicing].index.min()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "535729 535729\n",
      "742670 742670\n",
      "206941 206941\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train_orig.iloc[:slicing_index,:]\n",
    "y_train = y_train_orig.iloc[:slicing_index,:]\n",
    "X_eval = X_train_orig.iloc[slicing_index:,:]\n",
    "y_eval = y_train_orig.iloc[slicing_index:,:]\n",
    "print(len(X_train),len(y_train))\n",
    "print(len(X_train_orig),len(y_train_orig))\n",
    "print(len(X_eval),len(y_eval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def get_data(x_train,y_train):\n",
    "\tdata = []\n",
    "\tfor index,line in enumerate(x_train.T):\n",
    "\t\tnew_arr = []\n",
    "\t\tscore = y_train.iloc[index,-1]\n",
    "\t\tq_id = x_train.iloc[index,0]\n",
    "\t\tnew_arr.append(int(score))\n",
    "\t\tnew_arr.append(int(q_id))\n",
    "\t\tarr = x_train.iloc[index,2:]\n",
    "\t\tfor el in arr:\n",
    "\t\t\tnew_arr.append(el)\n",
    "\t\tdata.append(new_arr)\n",
    "\treturn np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lambdamart import LambdaMART\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start Fold 1\n",
      "Tree 0\n",
      "Tree 1\n",
      "Tree 2\n",
      "Tree 3\n",
      "Tree 4\n",
      "Tree 5\n",
      "Tree 6\n",
      "Tree 7\n",
      "Tree 8\n",
      "Tree 9\n",
      "Tree 10\n",
      "Tree 11\n",
      "Tree 12\n",
      "Tree 13\n",
      "Tree 14\n",
      "Tree 15\n",
      "Tree 16\n",
      "Tree 17\n",
      "Tree 18\n",
      "Tree 19\n",
      "Tree 20\n",
      "Tree 21\n",
      "Tree 22\n",
      "Tree 23\n",
      "Tree 24\n",
      "Tree 25\n",
      "Tree 26\n",
      "Tree 27\n",
      "Tree 28\n",
      "Tree 29\n",
      "Tree 30\n",
      "Tree 31\n",
      "Tree 32\n",
      "Tree 33\n",
      "Tree 34\n",
      "Tree 35\n",
      "Tree 36\n",
      "Tree 37\n",
      "Tree 38\n",
      "Tree 39\n",
      "Tree 40\n",
      "Tree 41\n",
      "Tree 42\n",
      "Tree 43\n",
      "Tree 44\n",
      "Tree 45\n",
      "Tree 46\n",
      "Tree 47\n",
      "Tree 48\n",
      "Tree 49\n",
      "Tree 50\n",
      "Tree 51\n",
      "Tree 52\n",
      "Tree 53\n",
      "Tree 54\n",
      "Tree 55\n",
      "Tree 56\n",
      "Tree 57\n",
      "Tree 58\n",
      "Tree 59\n",
      "Tree 60\n",
      "Tree 61\n",
      "Tree 62\n",
      "Tree 63\n",
      "Tree 64\n",
      "Tree 65\n",
      "Tree 66\n",
      "Tree 67\n",
      "Tree 68\n",
      "Tree 69\n",
      "Tree 70\n",
      "Tree 71\n",
      "Tree 72\n",
      "Tree 73\n",
      "Tree 74\n",
      "Tree 75\n",
      "Tree 76\n",
      "Tree 77\n",
      "Tree 78\n",
      "Tree 79\n",
      "Tree 80\n",
      "Tree 81\n",
      "Tree 82\n",
      "Tree 83\n",
      "Tree 84\n",
      "Tree 85\n",
      "Tree 86\n",
      "Tree 87\n",
      "Tree 88\n",
      "Tree 89\n",
      "Tree 90\n",
      "Tree 91\n",
      "Tree 92\n",
      "Tree 93\n",
      "Tree 94\n",
      "Tree 95\n",
      "Tree 96\n",
      "Tree 97\n",
      "Tree 98\n",
      "Tree 99\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "\ttotal_ndcg = 0.0\n",
    "\tfor i in [1]:\n",
    "\t\tprint('start Fold ' + str(i))\n",
    "\t\ttraining_data = get_data(X_train.iloc[:1000,:],y_train.iloc[:1000,:])\n",
    "\t\ttest_data = get_data(X_eval.iloc[:1000,:],y_eval.iloc[:1000,:])\n",
    "\t\tmodel = LambdaMART(training_data, 100, 0.01, 'sklearn')\n",
    "\t\tmodel.fit()\n",
    "\t\t#model.save('lambdamart_model_%d' % (i))\n",
    "\t\t# model = LambdaMART()\n",
    "\t\t# model.load('lambdamart_model.lmart')\n",
    "\t\t#average_ndcg, predicted_scores = model.validate(test_data, 10)\n",
    "\t\t#print(average_ndcg)\n",
    "\t\t#total_ndcg += average_ndcg\n",
    "\t#total_ndcg /= 5.0\n",
    "\t#print('Original average ndcg at 10 is: ' + str(total_ndcg))\n",
    "\t#total_ndcg = 0.0\n",
    "\treturn model\n",
    "\t'''for i in [1,2,3,4,5]:\n",
    "\t\tprint('start Fold ' + str(i))\n",
    "\t\ttraining_data = get_data(X_train,y_train)\n",
    "\t\ttest_data = get_data(X_eval,y_eval)\n",
    "\t\tmodel = LambdaMART(training_data, 300, 0.001, 'original')\n",
    "\t\tmodel.fit()\n",
    "\t\tmodel.save('lambdamart_model_sklearn_%d' % (i))\n",
    "\t\t# model = LambdaMART()\n",
    "\t\t# model.load('lambdamart_model.lmart')\n",
    "\t\taverage_ndcg, predicted_scores = model.validate(test_data, 10)\n",
    "\t\tprint(average_ndcg)\n",
    "\t\ttotal_ndcg += average_ndcg\n",
    "\ttotal_ndcg /= 5.0\n",
    "\tprint( 'Sklearn average ndcg at 10 is: ' + str(total_ndcg))'''\n",
    "model = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_test_data(x_train):\n",
    "\tdata = []\n",
    "\tfor index,line in enumerate(x_train.T):\n",
    "\t\tnew_arr = []\n",
    "\t\tq_id = x_train.iloc[index,0]\n",
    "\t\tnew_arr.append(int(q_id))\n",
    "\t\tarr = x_train.iloc[index,2:]\n",
    "\t\tfor el in arr:\n",
    "\t\t\tnew_arr.append(el)\n",
    "\t\tdata.append(new_arr)\n",
    "\treturn np.array(data)\n",
    "test_data = get_test_data(X_eval.iloc[:10,:])\n",
    "model.predict(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "score = spearmanr(y_test_local[\"y\"], y_test_local_pred[\"value\"])[0] * 100\n",
    "print(f\"Spearman's correlation {score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
